{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing the network that recognizes handwritten digits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this code, the list sizes contains the number of neurons in the respective layers. So, for example, if we want to create a Network object with 2 neurons in the first layer, 3 neurons in the second layer, and 1 neuron in the final layer, we'd do this with the code:\n",
    "\n",
    "net = Network([2, 3, 1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Network(object):\n",
    "        def __init__(self, sizes):\n",
    "            self.num_layers = len(sizes)\n",
    "            self.sizes = sizes\n",
    "            self.biases = [np.random.randn(y, 1) for y in sizes[1:]]\n",
    "            self.weights = [np.random.randn(y, x) \n",
    "                            for x, y in zip(sizes[:-1], sizes[1:])]\n",
    "        def __repr__(self):\n",
    "            return f'Network(num_layers={self.num_layers}, sizes={self.sizes}, biases={self.biases}, weights={self.weights})'\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ith column in the weight matrix represents the weights leading out of the ith neuron onto the next layer. In our example we have a 2-3-1 network so for the first element we have two colums, in the first column each row represent the weights from the first neuron to every neuron in the second layer.\n",
    "\n",
    "So in the matrix w wi,j correspond to the weigth from the jith neuron in the previous layer to the ith neuron in the current layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(z):\n",
    "    return 1.0/(1.0+np.exp(-z))\n",
    "\n",
    "# PRECOND : a is a (n,1) numpy array\n",
    "def feedforward(self, a):\n",
    "    \"\"\"Return the output of the network if \"a\" is input.\"\"\"\n",
    "    #  Give a tuple of (biais, weight) for each layer\n",
    "    for b, w in zip(self.biases, self.weights):\n",
    "        a = sigmoid(np.dot(w, a)+b)\n",
    "        return a\n",
    "\n",
    "Network.feedforward = feedforward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 13)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m<tokenize>:13\u001b[0;36m\u001b[0m\n\u001b[0;31m    if test_data: n_test = len(test_data)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "\n",
    "\n",
    "def SGD(self, training_data, epochs, mini_batch_size, eta, test_data=None):\n",
    "        \"\"\"Train the neural network using mini-batch stochastic\n",
    "        gradient descent.  The \"training_data\" is a list of tuples\n",
    "        \"(x, y)\" representing the training inputs and the desired\n",
    "        outputs.  The other non-optional parameters are\n",
    "        self-explanatory.  If \"test_data\" is provided then the\n",
    "        network will be evaluated against the test data after each\n",
    "        epoch, and partial progress printed out.  This is useful for\n",
    "        tracking progress, but slows things down substantially.\"\"\"\n",
    "    if test_data: n_test = len(test_data)\n",
    "\n",
    "    n = len(training_data)\n",
    "    for j in range(epochs):\n",
    "        random.shuffle(training_data)\n",
    "        mini_batches = training_data[k:k+mini_batch_size]\n",
    "        for k in range(0, n, mini_batch_size):\n",
    "            self.update_mini_batch(mini_batch, eta) # apply one step of gradient descent\n",
    "        if test_data:\n",
    "            print(\"Epoch {0}: {1} / {2}\".format(j, self.evaluate(test_data), n_test))\n",
    "        else:\n",
    "            print(\"Epoch {0} complete\".format(j))\n",
    "\n",
    "Network.SGD = SGD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "* training data is a list of tuples (x,y) where x is the input and y the expected output\n",
    "* epochs is the number \n",
    "* eta represents the learning rate\n",
    "* test_data is used to evaluate the network periodically during training, and is not used for training itself it slows down the process\n",
    "\n",
    "1. Shuffle the training data in order to get different batches each epochs\n",
    "2. For each mini batch we apply a single step of gradient descent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_mini_batch(self, mini_batch, eta):\n",
    "        \"\"\"Update the network's weights and biases by applying\n",
    "        gradient descent using backpropagation to a single mini batch.\n",
    "        The \"mini_batch\" is a list of tuples \"(x, y)\", and \"eta\"\n",
    "        is the learning rate.\"\"\"\n",
    "\n",
    "        # Initialize\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases] \n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        for x, y in mini_batch: \n",
    "            delta_nabla_b, delta_nabla_w = self.backprop(x, y) # compute gradient for each sample\n",
    "            # Update I\n",
    "            nabla_b = [nb+dnb for nb, dnb in zip(nabla_b, delta_nabla_b)]\n",
    "            nabla_w = [nw+dnw for nw, dnw in zip(nabla_w, delta_nabla_w)]\n",
    "        # Update II\n",
    "        self.weights = [w-(eta/len(mini_batch))*nw \n",
    "                        for w, nw in zip(self.weights, nabla_w)]\n",
    "        self.biases = [b-(eta/len(mini_batch))*nb \n",
    "                       for b, nb in zip(self.biases, nabla_b)]\n",
    "        \n",
    "Network.update_mini_batch = update_mini_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rappel : \n",
    "En considerant x un vecteur de 28*28 éléments dans chaque elements une intensité de gris. y(x) la valeur attendue de x comme un vecteur de type (0,0,0,1,0,0,0,0,0) represente 4 on considere la fonction de cout (MSE) \n",
    "\n",
    "  \n",
    "\n",
    "$$\n",
    "C(w,b) \\equiv \\frac{1}{2n} \\sum_x \\| y(x) - a\\|^2\n",
    "$$\n",
    "avec a la valeur sortie de l’algorithme pour une entrée x avec des poids w et des biais b.\n",
    "1. Initialize : $\\nabla b \\rightarrow 0 \\quad \\nabla w \\rightarrow 0 $\n",
    "2. Compute Gradient : \n",
    "3. Update I : $ \\nabla b \\rightarrow \\nabla b + \\delta \\nabla b \\quad \\nabla w \\rightarrow \\nabla w + \\delta \\nabla w $\n",
    "4. Update II : $w \\rightarrow w - \\frac{\\eta}{m} \\sum_x \\nabla w_x \\quad b \\rightarrow b - \\frac{\\eta}{m} \\sum_x \\nabla b_x$\n",
    "\n",
    "where $m$ is the size of the mini batch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backprop(self, x, y):\n",
    "        \"\"\"Return a tuple ``(nabla_b, nabla_w)`` representing the\n",
    "        gradient for the cost function C_x.  ``nabla_b`` and\n",
    "        ``nabla_w`` are layer-by-layer lists of numpy arrays, similar\n",
    "        to ``self.biases`` and ``self.weights``.\"\"\"\n",
    "        nabla_b = [np.zeros(b.shape) for b in self.biases]\n",
    "        nabla_w = [np.zeros(w.shape) for w in self.weights]\n",
    "        # feedforward\n",
    "        activation = x\n",
    "        activations = [x] # list to store all the activations, layer by layer\n",
    "        zs = [] # list to store all the z vectors, layer by layer\n",
    "        for b, w in zip(self.biases, self.weights):\n",
    "            z = np.dot(w, activation)+b\n",
    "            zs.append(z)\n",
    "            activation = sigmoid(z)\n",
    "            activations.append(activation)\n",
    "        # backward pass\n",
    "        delta = self.cost_derivative(activations[-1], y) * sigmoid_prime(zs[-1])\n",
    "        nabla_b[-1] = delta\n",
    "        nabla_w[-1] = np.dot(delta, activations[-2].transpose())\n",
    "        # Note that the variable l in the loop below is used a little\n",
    "        # differently to the notation in Chapter 2 of the book.  Here,\n",
    "        # l = 1 means the last layer of neurons, l = 2 is the\n",
    "        # second-last layer, and so on.  It's a renumbering of the\n",
    "        # scheme in the book, used here to take advantage of the fact\n",
    "        # that Python can use negative indices in lists.\n",
    "        for l in xrange(2, self.num_layers):\n",
    "            z = zs[-l]\n",
    "            sp = sigmoid_prime(z)\n",
    "            delta = np.dot(self.weights[-l+1].transpose(), delta) * sp\n",
    "            nabla_b[-l] = delta\n",
    "            nabla_w[-l] = np.dot(delta, activations[-l-1].transpose())\n",
    "        return (nabla_b, nabla_w)\n",
    "\n",
    "def cost_derivative(self, output_activations, y):\n",
    "        \"\"\"Return the vector of partial derivatives \\partial C_x /\n",
    "        \\partial a for the output activations.\"\"\"\n",
    "        return (output_activations-y)\n",
    "def sigmoid_prime(z):\n",
    "    \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "    return sigmoid(z)*(1-sigmoid(z))\n",
    "\n",
    "Network.backprop = backprop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explanation of the backpropagation algorithm : https://www.youtube.com/watch?v=Ilg3gGewQ5U "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Network() takes no arguments",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[50], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mmnist_loader\u001b[39;00m\n\u001b[1;32m      2\u001b[0m training_data, validation_data, test_data \u001b[38;5;241m=\u001b[39m mnist_loader\u001b[38;5;241m.\u001b[39mload_data_wrapper()\n\u001b[0;32m----> 3\u001b[0m net \u001b[38;5;241m=\u001b[39m \u001b[43mNetwork\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m784\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m30\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m net\u001b[38;5;241m.\u001b[39mSGD(training_data, \u001b[38;5;241m30\u001b[39m, \u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m3.0\u001b[39m, test_data\u001b[38;5;241m=\u001b[39mtest_data)\n",
      "\u001b[0;31mTypeError\u001b[0m: Network() takes no arguments"
     ]
    }
   ],
   "source": [
    "import mnist_loader\n",
    "training_data, validation_data, test_data = mnist_loader.load_data_wrapper()\n",
    "net = Network([784, 30, 10])\n",
    "net.SGD(training_data, 30, 10, 3.0, test_data=test_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
